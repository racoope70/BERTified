{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/racoope70/BERTified/blob/main/Results_Rebalance_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBnArpas7twX"
   },
   "outputs": [],
   "source": [
    "!pip -q install nbformat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZh_Aj5CZS3k"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, get_scheduler\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from google.colab import drive\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "import gc  # Import garbage collection for memory management\n",
    "from huggingface_hub import notebook_login\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11784,
     "status": "ok",
     "timestamp": 1766456480046,
     "user": {
      "displayName": "Richard Cooper",
      "userId": "11337060935753111062"
     },
     "user_tz": 300
    },
    "id": "xUEvjRE05Yl4",
    "outputId": "b26e4a06-0448-47f6-bca7-10b4f6ce9c75"
   },
   "outputs": [],
   "source": [
    "# Dependencies (Colab only)\n",
    "!pip install transformers pandas scikit-learn imbalanced-learn torch torchvision torchaudio matplotlib seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NPrtWJHL5lSZ"
   },
   "outputs": [],
   "source": [
    "# Load IMDb Dataset\n",
    "def load_imdb_dataset(base_path):\n",
    "    \"\"\"\n",
    "    Loads the IMDb dataset from given directory structure and returns a DataFrame with reviews and sentiments.\n",
    "    \"\"\"\n",
    "    reviews = []\n",
    "    sentiments = []\n",
    "\n",
    "    for split in ['train', 'test']:\n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            path = os.path.join(base_path, split, sentiment)\n",
    "            for file_name in os.listdir(path):\n",
    "                with open(os.path.join(path, file_name), 'r', encoding='utf-8') as file:\n",
    "                    reviews.append(file.read())\n",
    "                    sentiments.append(1 if sentiment == 'pos' else 0)\n",
    "\n",
    "    return pd.DataFrame({'review': reviews, 'sentiment': sentiments})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 110028,
     "status": "ok",
     "timestamp": 1766456590121,
     "user": {
      "displayName": "Richard Cooper",
      "userId": "11337060935753111062"
     },
     "user_tz": 300
    },
    "id": "mlsfTrma5yWv",
    "outputId": "c176617b-63ea-47d9-8a7e-d1375d7d3d4b"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABVPf2c450yy"
   },
   "outputs": [],
   "source": [
    "# Define paths for saving datasets and results\n",
    "base_path = '/content/drive/MyDrive/aclImdb'\n",
    "csv_file_path = '/content/drive/MyDrive/aclImdb_reviews.csv'\n",
    "results_dir = '/content/drive/MyDrive/sentiment_analysis_results'  # Define the directory for saving results\n",
    "\n",
    "# Ensure the results directory exists\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "if os.path.exists(csv_file_path):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "else:\n",
    "    df = load_imdb_dataset(base_path)\n",
    "    df.to_csv(csv_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-QKxzd-6AF4"
   },
   "outputs": [],
   "source": [
    "# Fixed-size sample for faster iteration (reproducible)\n",
    "data = df.sample(10000, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1835,
     "status": "ok",
     "timestamp": 1766456596675,
     "user": {
      "displayName": "Richard Cooper",
      "userId": "11337060935753111062"
     },
     "user_tz": 300
    },
    "id": "vreqHUyu6DKk",
    "outputId": "1d1ea188-f6d3-4825-8ddd-f6294de7a437"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into training and test sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    data['review'], data['sentiment'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Free up memory after splitting\n",
    "del df, data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9YHFMIdk6Tkm"
   },
   "outputs": [],
   "source": [
    "# Optional: Hugging Face authentication (only if rate-limited)\n",
    "# This allows for better access and avoids rate limits when downloading public models/datasets\n",
    "#notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Krf9mq-M6XMq"
   },
   "outputs": [],
   "source": [
    "# Batch tokenization (memory-friendly)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_data(texts, tokenizer, batch_size=64):\n",
    "    \"\"\"\n",
    "    Tokenizes input texts in smaller batches to optimize memory usage.\n",
    "    \"\"\"\n",
    "    tokenized_data = {\"input_ids\": [], \"attention_mask\": []}\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size].tolist()\n",
    "        encodings = tokenizer(batch, truncation=True, padding=True, max_length=256)\n",
    "        tokenized_data[\"input_ids\"].extend(encodings[\"input_ids\"])\n",
    "        tokenized_data[\"attention_mask\"].extend(encodings[\"attention_mask\"])\n",
    "    return tokenized_data\n",
    "\n",
    "train_encodings = tokenize_data(train_texts, tokenizer)\n",
    "test_encodings = tokenize_data(test_texts, tokenizer)\n",
    "\n",
    "train_texts_list = train_texts.tolist()\n",
    "train_labels_list = train_labels.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29756,
     "status": "ok",
     "timestamp": 1766456674054,
     "user": {
      "displayName": "Richard Cooper",
      "userId": "11337060935753111062"
     },
     "user_tz": 300
    },
    "id": "gKBc3XTvYcEC",
    "outputId": "d4a0c334-917b-4a01-b419-862de7165273"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset wrapper for Hugging Face Trainer\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for sentiment analysis.\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "train_texts_list = train_texts.tolist()  # Ensure train_texts is a list\n",
    "train_labels_list = train_labels.tolist()\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "train_texts_resampled, train_labels_resampled = ros.fit_resample(\n",
    "    pd.DataFrame({'text': train_texts_list}),  # Use a DataFrame to keep structure\n",
    "    train_labels_list\n",
    ")\n",
    "\n",
    "train_encodings_resampled = tokenizer(\n",
    "    train_texts_resampled['text'].tolist(), truncation=True, padding=True, max_length=128\n",
    ")\n",
    "\n",
    "train_dataset = SentimentDataset(train_encodings_resampled, list(train_labels_resampled))\n",
    "\n",
    "# Prepare testing dataset (unchanged)\n",
    "test_dataset = SentimentDataset(test_encodings, list(test_labels))\n",
    "\n",
    "# Free memory after creating datasets\n",
    "del train_encodings, test_encodings, train_labels, test_labels\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105,
     "referenced_widgets": [
      "52d066250539416795fe31bcd38a7e45",
      "8636740f1e7a4b29bbfa5aae460179bb",
      "8f5fbd61aceb4faf8e901f5c1619291c",
      "9d5ef873c7f44d34b946dbb7ed8da0db",
      "6b777eba36774839a57915b4de8ade86",
      "9b33cc7163564bd395a4e0cd95276cb6",
      "4080813fec22452ead8ca6d21d5d983a",
      "b5f3a3deb1fe420aa601448cab835b09",
      "cc53b60690ca4c9085c4d47bee28faf2",
      "83347fe72830466aa75695be0523afe7",
      "1da8dd04838540eeb222546479e02f5b"
     ]
    },
    "executionInfo": {
     "elapsed": 14585,
     "status": "ok",
     "timestamp": 1766456688635,
     "user": {
      "displayName": "Richard Cooper",
      "userId": "11337060935753111062"
     },
     "user_tz": 300
    },
    "id": "pdsE-7C87tOk",
    "outputId": "901afbbe-b031-4301-95c0-cd927acc9060"
   },
   "outputs": [],
   "source": [
    "# Initialize BERT sequence classifier (GPU-enabled if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pe51FQW863_t"
   },
   "outputs": [],
   "source": [
    "# Training configuration (evaluation + checkpointing)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=results_dir,  # Save checkpoints to Google Drive\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,  # Limit the number of saved checkpoints\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.1,\n",
    "    logging_dir=os.path.join(results_dir, \"logs\"),  # Log directory\n",
    "    fp16=torch.cuda.is_available(),  # Mixed precision training\n",
    "    load_best_model_at_end=True,  # Load the best model\n",
    "    report_to=\"tensorboard\",\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1111,
     "status": "ok",
     "timestamp": 1766456689753,
     "user": {
      "displayName": "Richard Cooper",
      "userId": "11337060935753111062"
     },
     "user_tz": 300
    },
    "id": "ScSJutf66_vv",
    "outputId": "070be663-2628-40c3-cf2f-4abf6748c0ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure Trainer with checkpointing and evaluation\n",
    "# Optimizer and learning-rate schedule (accounts for gradient accumulation)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "steps_per_epoch = (len(train_dataset) // training_args.per_device_train_batch_size) + 1\n",
    "optimizer_steps_per_epoch = max(1, steps_per_epoch // training_args.gradient_accumulation_steps)\n",
    "num_training_steps = int(optimizer_steps_per_epoch * training_args.num_train_epochs)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QdzG9dwyqlJi"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    optimizers=(optimizer, lr_scheduler),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "qh6s_W_y7gcz",
    "outputId": "43eab434-7e80-445c-f0dd-491a53639c1e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='636' max='635' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [635/635 08:09, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.276526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.237438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.249830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.258300</td>\n",
       "      <td>0.276889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.258300</td>\n",
       "      <td>0.324814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Begin fine-tuning\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DyKP-zF8CKs"
   },
   "outputs": [],
   "source": [
    "# Evaluate model on held-out test set\n",
    "preds = trainer.predict(test_dataset).predictions\n",
    "\n",
    "test_labels_tensor = torch.tensor(list(test_dataset.labels)).to(torch.device(\"cpu\"))\n",
    "binary_preds = torch.argmax(torch.tensor(preds), dim=1)\n",
    "accuracy = accuracy_score(test_labels_tensor.numpy(), binary_preds.numpy())\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WxvO0l2B1dHo"
   },
   "outputs": [],
   "source": [
    "# Obtain predictions and true labels\n",
    "output = trainer.predict(test_dataset)  # Get predictions and metrics from the model\n",
    "predictions = output.predictions  # Raw predictions from the model\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_labels = torch.argmax(torch.tensor(predictions), dim=1).numpy()\n",
    "\n",
    "# Extract true labels from the test dataset\n",
    "true_labels = [example['labels'] for example in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bej-WBP0wCOF"
   },
   "outputs": [],
   "source": [
    "# Summary metrics on held-out test set\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels, average=\"binary\")\n",
    "recall = recall_score(true_labels, predicted_labels, average=\"binary\")\n",
    "f1 = f1_score(true_labels, predicted_labels, average=\"binary\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "\n",
    "# Detailed per-class breakdown\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predicted_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "egRUrGxRPe0a"
   },
   "outputs": [],
   "source": [
    "# Threshold-based diagnostics (ROC, PR, confusion matrix)\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, roc_curve, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert logits to positive-class probabilities for threshold-based diagnostics\n",
    "probs = torch.softmax(torch.tensor(predictions), dim=1).numpy()\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "disp.plot()\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display ROC-AUC score\n",
    "roc_auc = roc_auc_score(true_labels, probs[:, 1])\n",
    "print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n",
    "\n",
    "# ROC-AUC and ROC curve\n",
    "fpr, tpr, _ = roc_curve(true_labels, probs[:, 1])\n",
    "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Precisionâ€“Recall curve (positive class)\n",
    "precision, recall, _ = precision_recall_curve(true_labels, probs[:, 1])\n",
    "plt.plot(recall, precision, label=\"Precision-Recall Curve\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xinaAvh9BTRl"
   },
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from pathlib import Path\n",
    "\n",
    "nb_path = Path(\"/content/drive/MyDrive/imdb_bert_sentiment_rebalanced.ipynb\")\n",
    "\n",
    "nb = nbformat.read(str(nb_path), as_version=4)\n",
    "\n",
    "# Remove notebook-level widgets metadata (fixes GitHub \"Invalid Notebook\")\n",
    "nb.metadata.pop(\"widgets\", None)\n",
    "\n",
    "nbformat.write(nb, str(nb_path))\n",
    "print(\"Updated in place (same filename):\", nb_path)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
