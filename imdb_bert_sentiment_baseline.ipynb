{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/racoope70/BERTified/blob/main/imdb_bert_sentiment_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KETsHxJHqnw8"
      },
      "outputs": [],
      "source": [
        "!pip -q install -U transformers datasets \"huggingface_hub[hf_xet]\" scikit-learn matplotlib seaborn\n",
        "!pip -q install pandas==2.2.2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZh_Aj5CZS3k"
      },
      "outputs": [],
      "source": [
        "# Core libs\n",
        "import os\n",
        "import gc\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Modeling + training\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Evaluation + splits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        ")\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPrtWJHL5lSZ"
      },
      "outputs": [],
      "source": [
        "# Load IMDb dataset (Drive-based)\n",
        "def load_imdb_dataset(base_path):\n",
        "    \"\"\"\n",
        "    Loads the IMDb dataset from given directory structure and returns a DataFrame with reviews and sentiments.\n",
        "    \"\"\"\n",
        "    reviews = []\n",
        "    sentiments = []\n",
        "\n",
        "    for split in ['train', 'test']:\n",
        "        for sentiment in ['pos', 'neg']:\n",
        "            path = os.path.join(base_path, split, sentiment)\n",
        "            for file_name in os.listdir(path):\n",
        "                with open(os.path.join(path, file_name), 'r', encoding='utf-8') as file:\n",
        "                    reviews.append(file.read())\n",
        "                    sentiments.append(1 if sentiment == 'pos' else 0)\n",
        "\n",
        "    return pd.DataFrame({'review': reviews, 'sentiment': sentiments})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlsfTrma5yWv",
        "outputId": "0b2f0c0d-7d40-4525-d455-e173724efcf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABVPf2c450yy"
      },
      "outputs": [],
      "source": [
        "base_path = '/content/drive/MyDrive/aclImdb'\n",
        "csv_file_path = '/content/drive/MyDrive/aclImdb_reviews.csv'\n",
        "results_dir = '/content/drive/MyDrive/sentiment_analysis_results'  # Define the directory for saving results\n",
        "\n",
        "if not os.path.exists(results_dir):\n",
        "    os.makedirs(results_dir)\n",
        "\n",
        "if os.path.exists(csv_file_path):\n",
        "    df = pd.read_csv(csv_file_path)\n",
        "else:\n",
        "    df = load_imdb_dataset(base_path)\n",
        "    df.to_csv(csv_file_path, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-QKxzd-6AF4"
      },
      "outputs": [],
      "source": [
        "# Fixed-size sample for faster iteration (reproducible)\n",
        "data = df.sample(10000, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vreqHUyu6DKk",
        "outputId": "892e7ab2-a596-44b6-c8c1-83b0d60ee914"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    data['review'], data['sentiment'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Free up memory after splitting\n",
        "del df, data\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "82447c76f5ca4bc49482c9fd536cd3bf",
            "72cac732b4da4ce58cf09e9bc9a01576",
            "0cec8b62b5084706b1d484fec76bd2fa",
            "d5a9f57d81b14919b5b43ca787282a38",
            "25ca6077f6ea4545ac8b820a504b1d2b",
            "a6e699690809496fb4dce90d1451dd27",
            "079de869030e4a89864a9741c6264b37",
            "01cd9edb92664662bde15b89a90e581e",
            "b56a3a332af542ed81187c3f0f84449c",
            "bc419a5f700043368f171079995da23a",
            "9d5ecc5d7cab4744a6e8b58cbe94437e",
            "3d560c5f6b4d4777b3689db5b6d9b856",
            "c0b2536ae56e4cbfb6cf3d9104649682",
            "4f02100ad7224d57b7fecff3f75fc7ac",
            "d66d749d634b4b5e88cf8cfc1f21468d",
            "2e05a824870a4f8b94c24c341f645e3e",
            "737c426617a7445fb86841cfd895295f",
            "f390d8e19ab640e5b0039302f58a3d9d",
            "492aa0773a2a4c57ad8d352ce5bb6cd1",
            "64a9a0827ac540918cc2ad198da7fc99"
          ]
        },
        "id": "9YHFMIdk6Tkm",
        "outputId": "6b6c873d-2c6a-430c-8a19-f29776378314"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Optional: Hugging Face authentication (only if rate-limited)\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Krf9mq-M6XMq",
        "outputId": "4ab56a78-306e-4e51-8f55-3951e08266c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Tokenize text in batches (memory-friendly)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_data(texts, tokenizer, batch_size=64):\n",
        "    \"\"\"\n",
        "    Tokenizes input texts in smaller batches to optimize memory usage.\n",
        "    \"\"\"\n",
        "    tokenized_data = {\"input_ids\": [], \"attention_mask\": []}\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i + batch_size].tolist()\n",
        "        encodings = tokenizer(batch, truncation=True, padding=True, max_length=256)\n",
        "        tokenized_data[\"input_ids\"].extend(encodings[\"input_ids\"])\n",
        "        tokenized_data[\"attention_mask\"].extend(encodings[\"attention_mask\"])\n",
        "    return tokenized_data\n",
        "\n",
        "train_encodings = tokenize_data(train_texts, tokenizer)\n",
        "test_encodings = tokenize_data(test_texts, tokenizer)\n",
        "\n",
        "del train_texts, test_texts\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WasCUuV6xue",
        "outputId": "49239acd-4107-4cc6-b049-c90e1e244bd8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Dataset wrapper for Hugging Face Trainer\n",
        "class SentimentDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Custom Dataset class for sentiment analysis.\n",
        "    \"\"\"\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "train_dataset = SentimentDataset(train_encodings, list(train_labels))\n",
        "test_dataset = SentimentDataset(test_encodings, list(test_labels))\n",
        "\n",
        "del train_encodings, test_encodings, train_labels, test_labels\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdsE-7C87tOk",
        "outputId": "9e1c32d6-8a60-48d5-b764-68211157bf0c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Model setup (BERT + GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pe51FQW863_t"
      },
      "outputs": [],
      "source": [
        "# Training configuration (evaluation + checkpointing)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=results_dir,  # Save checkpoints to Google Drive\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=3,\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=4,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.1,\n",
        "    logging_dir=os.path.join(results_dir, \"logs\"),\n",
        "    logging_strategy=\"epoch\",\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    load_best_model_at_end=True,\n",
        "    seed=42,\n",
        "    data_seed=42,\n",
        "\n",
        "    # Github reduce widget output risk\n",
        "    disable_tqdm=True,   # disable tqdm progress bars to avoid widget metadata\n",
        "    report_to=[],        # disable TensorBoard reporting to prevent rich output metadata\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScSJutf66_vv",
        "outputId": "8ff87486-73f6-42a0-8c0f-7833335d0fe5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "120"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Trainer setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh6s_W_y7gcz",
        "outputId": "a76443f5-f09d-48a4-f899-4d1094b4c939"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4403, 'grad_norm': 6.543554782867432, 'learning_rate': 8.016e-06, 'epoch': 1.0}\n",
            "{'eval_loss': 0.27512091398239136, 'eval_runtime': 6.8796, 'eval_samples_per_second': 290.715, 'eval_steps_per_second': 9.158, 'epoch': 1.0}\n",
            "{'loss': 0.2435, 'grad_norm': 4.935958385467529, 'learning_rate': 6.0160000000000005e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.25124916434288025, 'eval_runtime': 6.7897, 'eval_samples_per_second': 294.564, 'eval_steps_per_second': 9.279, 'epoch': 2.0}\n",
            "{'loss': 0.1844, 'grad_norm': 3.713592529296875, 'learning_rate': 4.016e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.2625543773174286, 'eval_runtime': 7.0277, 'eval_samples_per_second': 284.586, 'eval_steps_per_second': 8.964, 'epoch': 3.0}\n",
            "{'loss': 0.1468, 'grad_norm': 5.988775730133057, 'learning_rate': 2.0160000000000003e-06, 'epoch': 4.0}\n",
            "{'eval_loss': 0.25764840841293335, 'eval_runtime': 6.8943, 'eval_samples_per_second': 290.095, 'eval_steps_per_second': 9.138, 'epoch': 4.0}\n",
            "{'loss': 0.1216, 'grad_norm': 4.5121002197265625, 'learning_rate': 1.6e-08, 'epoch': 5.0}\n",
            "{'eval_loss': 0.26095905900001526, 'eval_runtime': 6.9031, 'eval_samples_per_second': 289.723, 'eval_steps_per_second': 9.126, 'epoch': 5.0}\n",
            "{'train_runtime': 535.1912, 'train_samples_per_second': 74.74, 'train_steps_per_second': 1.168, 'train_loss': 0.22730752410888672, 'epoch': 5.0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=625, training_loss=0.22730752410888672, metrics={'train_runtime': 535.1912, 'train_samples_per_second': 74.74, 'train_steps_per_second': 1.168, 'train_loss': 0.22730752410888672, 'epoch': 5.0})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Begin fine-tuning\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Py3W7q8b9SMZ",
        "outputId": "804392dd-507f-4589-9dec-068f6c02294c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.8970\n",
            "Precision: 0.8717\n",
            "Recall:    0.9296\n",
            "F1 Score:  0.8997\n"
          ]
        }
      ],
      "source": [
        "# Run evaluation\n",
        "preds = trainer.predict(test_dataset).predictions\n",
        "\n",
        "# Convert predictions and labels to CPU numpy arrays\n",
        "y_true = torch.tensor(test_dataset.labels).cpu().numpy()\n",
        "y_pred = torch.argmax(torch.tensor(preds), dim=1).cpu().numpy()\n",
        "\n",
        "# Summary classification metrics\n",
        "accuracy  = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall    = recall_score(y_true, y_pred)\n",
        "f1        = f1_score(y_true, y_pred)\n",
        "\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "7E2bBobmpvaw",
        "outputId": "269cfec9-e886-4e85-9557-e1ef5dda7cae"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/imdb_bert_sentiment_baseline_clean.ipynb'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nbformat/__init__.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(fp, as_version, capture_validation_error, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'read'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1710248800.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnb_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/imdb_bert_sentiment_baseline_clean.ipynb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnbformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Remove top-level widget metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nbformat/__init__.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(fp, as_version, capture_validation_error, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: PTH123\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_validation_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/imdb_bert_sentiment_baseline_clean.ipynb'"
          ]
        }
      ],
      "source": [
        "# --- GitHub compatibility fix: remove widget metadata ---\n",
        "!pip -q install nbformat\n",
        "\n",
        "import nbformat\n",
        "from pathlib import Path\n",
        "\n",
        "# Source notebook (change this)\n",
        "nb_path = Path(\"/content/drive/MyDrive/imdb_bert_sentiment_baseline_clean.ipynb\")\n",
        "\n",
        "nb = nbformat.read(str(nb_path), as_version=4)\n",
        "\n",
        "# Remove top-level widget metadata\n",
        "nb.metadata.pop(\"widgets\", None)\n",
        "\n",
        "WIDGET_MIMES = {\n",
        "    \"application/vnd.jupyter.widget-view+json\",\n",
        "    \"application/vnd.jupyter.widget-state+json\",\n",
        "}\n",
        "\n",
        "# Remove widget MIME outputs from code cells\n",
        "for cell in nb.cells:\n",
        "    if cell.get(\"cell_type\") != \"code\":\n",
        "        continue\n",
        "    for output in cell.get(\"outputs\", []):\n",
        "        data = output.get(\"data\")\n",
        "        if isinstance(data, dict):\n",
        "            for mime in WIDGET_MIMES:\n",
        "                data.pop(mime, None)\n",
        "\n",
        "# Save as a new cleaned copy\n",
        "clean_path = nb_path.with_name(f\"{nb_path.stem}_github.ipynb\")\n",
        "nbformat.write(nb, str(clean_path))\n",
        "\n",
        "print(f\"Saved GitHub-friendly notebook: {clean_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSJQonBBww-C"
      },
      "outputs": [],
      "source": [
        "# Clean notebook so GitHub can render it (removes widget metadata + widget MIME outputs)\n",
        "!pip -q install nbformat\n",
        "\n",
        "import nbformat\n",
        "from pathlib import Path\n",
        "\n",
        "WIDGET_MIMES = {\n",
        "    \"application/vnd.jupyter.widget-view+json\",\n",
        "    \"application/vnd.jupyter.widget-state+json\",\n",
        "}\n",
        "\n",
        "def clean_notebook(src_path: str) -> str:\n",
        "    src = Path(src_path)\n",
        "    nb = nbformat.read(str(src), as_version=4)\n",
        "\n",
        "    # Drop notebook-level widget metadata\n",
        "    nb.metadata.pop(\"widgets\", None)\n",
        "\n",
        "    # Strip widget MIME payloads from outputs\n",
        "    for cell in nb.cells:\n",
        "        if cell.get(\"cell_type\") != \"code\":\n",
        "            continue\n",
        "        for out in cell.get(\"outputs\", []):\n",
        "            data = out.get(\"data\")\n",
        "            if isinstance(data, dict):\n",
        "                for mime in WIDGET_MIMES:\n",
        "                    data.pop(mime, None)\n",
        "\n",
        "    dst = src.with_name(f\"{src.stem}_github.ipynb\")\n",
        "    nbformat.write(nb, str(dst))\n",
        "    return str(dst)\n",
        "\n",
        "# CHANGE THIS to the real file you want to fix:\n",
        "cleaned = clean_notebook(\"/content/drive/MyDrive/imdb_bert_sentiment_baseline.ipynb\")\n",
        "print(\"Saved:\", cleaned)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}